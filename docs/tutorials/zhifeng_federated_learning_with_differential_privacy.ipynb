{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YRGLD1pOOrJ"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Federated Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "koW3R4ntOgLS"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coAumH42q9nz"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/federated_learning_with_differential_privacy\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.52.0/docs/tutorials/federated_learning_with_differential_privacy.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.52.0/docs/tutorials/federated_learning_with_differential_privacy.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/federated/docs/tutorials/federated_learning_with_differential_privacy.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grBmytrShbUE"
   },
   "source": [
    "# Differential Privacy in TFF\n",
    "\n",
    "This tutorial will demonstrate the recommended best practice for training models with user-level Differential Privacy using Tensorflow Federated. We will use the DP-SGD algorithm of [Abadi et al., \"Deep Learning with Differential Privacy\"](https://arxiv.org/abs/1607.00133) modified for user-level DP in a federated context in [McMahan et al., \"Learning Differentially Private Recurrent Language Models\"](https://arxiv.org/abs/1710.06963).\n",
    "\n",
    "Differential Privacy (DP) is a widely used method for bounding and quantifying the privacy leakage of sensitive data when performing learning tasks. Training a model with user-level DP guarantees that the model is unlikely to learn anything significant about the data of any individual, but can still (hopefully!) learn patterns that exist in the data of many clients.\n",
    "\n",
    "We will train a model on the federated EMNIST dataset. There is an inherent trade-off between utility and privacy, and it may be difficult to train a model with high privacy that performs as well as a state-of-the-art non-private model. For expediency in this tutorial, we will train for just 100 rounds, sacrificing some quality in order to demonstrate how to train with high privacy. If we used more training rounds, we could certainly have a somewhat higher-accuracy private model, but not as high as a model trained without DP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiq_MY4LopET"
   },
   "source": [
    "## Before we begin\n",
    "\n",
    "First, let us make sure the notebook is connected to a backend that has the relevant components compiled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Ubuntu 22.04 with conda 23.1.0, Python 3.9.16, and pip 23.0.1\n",
    "# pip install --upgrade tensorflow-federated\n",
    "# pip install --upgrade nest-asyncio\n",
    "# conda install ipykernel\n",
    "# python -m ipykernel install --user --name tff --display-name \"Python (tff)\"\n",
    "# \n",
    "\n",
    "# (dp-accounting is not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ke7EyuvG0Zyn"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "# !pip install --quiet --upgrade dp-accounting\n",
    "# !pip install --quiet --upgrade tensorflow-federated\n",
    "# !pip install --quiet --upgrade nest-asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cmp7VMf15aSu"
   },
   "source": [
    "Some imports we will need for the tutorial. We will use `tensorflow_federated`, the open-source framework for machine learning and other computations on decentralized data, as well as `dp_accounting`, an open-source library for analyzing differentially private algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rtgStTrNIId-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 12:21:10.035331: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 12:21:10.152014: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-28 12:21:10.152036: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-28 12:21:10.731310: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 12:21:10.731366: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 12:21:10.731372: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import dp_accounting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6eWsahmQpmi"
   },
   "source": [
    "Run the following \"Hello World\"\n",
    "example to make sure the TFF environment is correctly setup. If it doesn't work,\n",
    "please refer to the [Installation](../install.md) guide for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wjX3wmC-P1aE",
    "outputId": "b3e2cf36-a8f0-42af-84f0-24affeaad54a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 12:21:11.950811: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-28 12:21:11.950838: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-28 12:21:11.950854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-19-74): /proc/driver/nvidia/version does not exist\n",
      "2023-03-28 12:21:11.950963: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 12:21:11.951167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tff.federated_computation\n",
    "def hello_world():\n",
    "  return 'Hello, World!'\n",
    "\n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8xl6I2X9ObS"
   },
   "source": [
    "## Download and preprocess the federated EMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "EdY72mGKJqi0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "client_epochs_per_round = 2\n",
    "only_digits = False\n",
    "\n",
    "tf.random.set_seed(seed=1)\n",
    "\n",
    "def get_emnist_dataset():\n",
    "  emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(\n",
    "      only_digits=only_digits)\n",
    "\n",
    "  def rotation_fn(image):\n",
    "    def rand_degree():\n",
    "      upper = 5 * (math.pi / 180.0) # degrees -> radian\n",
    "      lower = -5 * (math.pi / 180.0)\n",
    "      return random.uniform(lower , upper)\n",
    "        \n",
    "    return tfa.image.rotate(image, rand_degree(), fill_value=1.0)\n",
    "\n",
    "#   def crop_fn(image):\n",
    "#     return tf.image.resize_with_crop_or_pad(\n",
    "#         image, target_height=image.shape[0], target_width=image.shape[1])\n",
    "\n",
    "  def normalize_fn(image):\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    image = (image - 0.9637) / 0.1597\n",
    "    return image\n",
    "\n",
    "  def element_fn(element):\n",
    "    return collections.OrderedDict(\n",
    "#         x = tf.expand_dims(element['pixels'], -1),\n",
    "        x=normalize_fn(\n",
    "            rotation_fn(\n",
    "                tf.expand_dims(element['pixels'], -1)\n",
    "            )\n",
    "        ),\n",
    "        y=element['label']\n",
    "    )\n",
    "\n",
    "  def preprocess_train_dataset(dataset):\n",
    "    # Use buffer_size same as the maximum client dataset size,\n",
    "    # 418 for Federated EMNIST\n",
    "    return (dataset.map(element_fn)\n",
    "                   .shuffle(buffer_size=418)\n",
    "                   .repeat(count=client_epochs_per_round)\n",
    "                   .batch(20, drop_remainder=False))\n",
    "#                    .batch(32, drop_remainder=False))\n",
    "\n",
    "  def preprocess_test_dataset(dataset):\n",
    "    return dataset.map(element_fn).batch(128, drop_remainder=False)\n",
    "\n",
    "  emnist_train = emnist_train.preprocess(preprocess_train_dataset)\n",
    "  emnist_test = preprocess_test_dataset(\n",
    "      emnist_test.create_tf_dataset_from_all_clients())\n",
    "\n",
    "  return emnist_train, emnist_test\n",
    "\n",
    "train_data, test_data = get_emnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12, shape=(), dtype=int32) (28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(36, shape=(), dtype=int32) (28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(40, shape=(), dtype=int32) (28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# first_client_id = train_data.client_ids[0]\n",
    "# first_client_dataset = train_data.create_tf_dataset_for_client(first_client_id)\n",
    "# it = iter(first_client_dataset)\n",
    "# first_batch = next(it)\n",
    "\n",
    "# for idx, image in enumerate(first_batch['x'][:3]):\n",
    "#     plt.subplot(1,2,1) \n",
    "#     print(first_batch['y'][idx], image.shape)\n",
    "#     plt.imshow(image, cmap='gray')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 1000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "virtual_client_cnt = 0\n",
    "virtual_global_dataset = {}\n",
    "physcial_clients_per_merge = 3\n",
    "\n",
    "physical_client_cnt = 0\n",
    "virtual_client_cnt = 0\n",
    "temp_physical_dataset_list = []\n",
    "# TODO: optimize with multiprocessing\n",
    "for client_id in train_data.client_ids:\n",
    "  physical_client_dataset = tfds.as_numpy(train_data.create_tf_dataset_for_client(client_id))\n",
    "  temp_physical_dataset_list.append(physical_client_dataset._ds)\n",
    "\n",
    "  if (physical_client_cnt + 1) % physcial_clients_per_merge == 0:\n",
    "    virtual_client_dataset = temp_physical_dataset_list[0]\n",
    "\n",
    "    for i in range(physcial_clients_per_merge - 1):\n",
    "        j = i + 1\n",
    "        virtual_client_dataset = virtual_client_dataset.concatenate(temp_physical_dataset_list[j])\n",
    "\n",
    "    virtual_global_dataset.update({\n",
    "        virtual_client_cnt: virtual_client_dataset\n",
    "    })\n",
    "    virtual_client_cnt += 1\n",
    "    temp_physical_dataset_list = []\n",
    "    if virtual_client_cnt == 1000:\n",
    "        break\n",
    "\n",
    "  physical_client_cnt += 1\n",
    "\n",
    "print(physical_client_cnt + 1, virtual_client_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2064\n"
     ]
    }
   ],
   "source": [
    "first_virtual_client_id = 0\n",
    "first_virtual_client_dataset = virtual_global_dataset[first_virtual_client_id]\n",
    "it = iter(first_virtual_client_dataset)\n",
    "\n",
    "first_virtual_client_dataset_size_after_repetition = 0\n",
    "for b in it:\n",
    "  first_virtual_client_dataset_size_after_repetition += len(b['x'])\n",
    "print(first_virtual_client_dataset_size_after_repetition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ntJ5coIJxS2"
   },
   "source": [
    "## Define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "YK_UGq_0KGMX"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "data_format = 'channels_last'\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=seed)\n",
    "\n",
    "def my_model_fn():\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(\n",
    "          32,\n",
    "          kernel_size=(3, 3),\n",
    "          activation='relu',\n",
    "          data_format=data_format,\n",
    "          input_shape=(28, 28, 1),\n",
    "          kernel_initializer=initializer),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=(2, 2), data_format=data_format),\n",
    "      tf.keras.layers.Conv2D(\n",
    "          64,\n",
    "          kernel_size=(3, 3),\n",
    "          activation='relu',\n",
    "          data_format=data_format,\n",
    "          kernel_initializer=initializer),\n",
    "      tf.keras.layers.Dropout(0.25),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(\n",
    "          128, activation='relu', kernel_initializer=initializer),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(\n",
    "          10 if only_digits else 62,\n",
    "          activation=tf.nn.softmax,\n",
    "          kernel_initializer=initializer),\n",
    "  ])\n",
    "#       tf.keras.layers.Reshape(input_shape=(28, 28, 1), target_shape=(28 * 28,)),\n",
    "#       tf.keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "#       tf.keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "#       tf.keras.layers.Dense(10)])\n",
    "  return tff.learning.models.from_keras_model(\n",
    "      keras_model=model,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      input_spec=test_data.element_spec,\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2jnGd-CKTYW"
   },
   "source": [
    "## Determine the noise sensitivity of the model.\n",
    "\n",
    "To get user-level DP guarantees, we must change the basic Federated Averaging algorithm in two ways. First, the clients' model updates must be clipped before transmission to the server, bounding the maximum influence of any one client. Second, the server must add enough noise to the sum of user updates before averaging to obscure the worst-case client influence.\n",
    "\n",
    "For clipping, we use the adaptive clipping method of [Andrew et al. 2021, Differentially Private Learning with Adaptive Clipping](https://arxiv.org/abs/1905.03871), so no clipping norm needs to be explicitly set.\n",
    "\n",
    "Adding noise will in general degrade the utility of the model, but we can control the amount of noise in the average update at each round with two knobs: the standard deviation of the Gaussian noise added to the sum, and the number of clients in the average. Our strategy will be to first determine how much noise the model can tolerate with a relatively small number of clients per round with acceptable loss to model utility. Then to train the final model, we can increase the amount of noise in the sum, while proportionally scaling up the number of clients per round (assuming the dataset is large enough to support that many clients per round). This is unlikely to significantly affect model quality, since the only effect is to decrease variance due to client sampling (indeed we will verify that it does not in our case).\n",
    "\n",
    "To that end, we first train a series of models with 50 clients per round, with increasing amounts of noise. Specifically, we increase the \"noise_multiplier\" which is the ratio of the noise standard deviation to the clipping norm. Since we are using adaptive clipping, this means that the actual magnitude of the noise changes from round to round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "6-BZ_L4GMmXP"
   },
   "outputs": [],
   "source": [
    "total_clients = virtual_client_cnt\n",
    "# total_clients = len(train_data.client_ids)\n",
    "\n",
    "clients_per_thread = 5\n",
    "# clients_per_thread = 5\n",
    "# tff.backends.native.set_sync_local_cpp_execution_context(\n",
    "#     max_concurrent_computation_calls=total_clients / clients_per_thread)\n",
    "\n",
    "def train(rounds, noise_multiplier, clients_per_round, data_frame):\n",
    "  # Using the `dp_aggregator` here turns on differential privacy with adaptive\n",
    "  # clipping.\n",
    "  aggregation_factory = tff.learning.model_update_aggregator.dp_aggregator(\n",
    "      noise_multiplier, clients_per_round)\n",
    "\n",
    "  # We use Poisson subsampling which gives slightly tighter privacy guarantees\n",
    "  # compared to having a fixed number of clients per round. The actual number of\n",
    "  # clients per round is stochastic with mean clients_per_round.\n",
    "#   sampling_prob = clients_per_round / total_clients\n",
    "\n",
    "  # Build a federated averaging process.\n",
    "  # Typically a non-adaptive server optimizer is used because the noise in the\n",
    "  # updates can cause the second moment accumulators to become very large\n",
    "  # prematurely.\n",
    "  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n",
    "        my_model_fn,\n",
    "#         client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01),\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01, momentum=0.9),\n",
    "#         server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0),\n",
    "        model_aggregator=aggregation_factory)\n",
    "\n",
    "  eval_process = tff.learning.build_federated_evaluation(my_model_fn)\n",
    "    \n",
    "  # Training loop.\n",
    "  state = learning_process.initialize()\n",
    "  for round in range(rounds):\n",
    "    print(f\"Round {round}\")\n",
    "    if round % 1 == 0:\n",
    "      model_weights = learning_process.get_model_weights(state)\n",
    "      metrics = eval_process(model_weights, [test_data])['eval']\n",
    "      if round % 1 == 0:\n",
    "#       if round < 25 or round % 25 == 0:\n",
    "        print(f'Round {round:3d}: {metrics}')\n",
    "      data_frame = data_frame.append({'Round': round,\n",
    "                                      'NoiseMultiplier': noise_multiplier,\n",
    "                                      **metrics}, ignore_index=True)\n",
    "\n",
    "    # Sample clients for a round. Note that if your dataset is large and\n",
    "    # sampling_prob is small, it would be faster to use gap sampling.\n",
    "    sampled_clients = np.random.choice(\n",
    "        a=total_clients,\n",
    "        size=clients_per_round,\n",
    "        replace=False\n",
    "    )\n",
    "#     print(f\"Sampled clients: {sampled_clients}\")\n",
    "#     x = np.random.uniform(size=total_clients)\n",
    "#     sampled_clients = [\n",
    "#         train_data.client_ids[i] for i in range(total_clients)\n",
    "#         if x[i] < sampling_prob]\n",
    "    \n",
    "    sampled_train_data = [\n",
    "        virtual_global_dataset[client] for client in sampled_clients\n",
    "    ]\n",
    "#     sampled_train_data = [\n",
    "#         train_data.create_tf_dataset_for_client(client)\n",
    "#         for client in sampled_clients]\n",
    "\n",
    "    # Use selected clients for update.\n",
    "    result = learning_process.next(state, sampled_train_data)\n",
    "    state = result.state\n",
    "    metrics = result.metrics\n",
    "\n",
    "  model_weights = learning_process.get_model_weights(state)\n",
    "  metrics = eval_process(model_weights, [test_data])['eval']\n",
    "  print(f'Round {rounds:3d}: {metrics}')\n",
    "  data_frame = data_frame.append({'Round': rounds,\n",
    "                                  'NoiseMultiplier': noise_multiplier,\n",
    "                                  **metrics}, ignore_index=True)\n",
    "\n",
    "  return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "qcaBxl0AbLTQ",
    "outputId": "22cdad6b-3ce2-4db0-b0be-ba1fe9599f8c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with noise multiplier: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_495098/2449149110.py:32: DeprecationWarning: `tff.learning.build_federated_evaluation` is deprecated, use `tff.learning.algorithms.build_fed_eval` instead.\n",
      "  eval_process = tff.learning.build_federated_evaluation(my_model_fn)\n",
      "2023-03-28 15:07:55.854018: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:55.877488: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:55.883384: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:55.890676: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:55.906590: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:55.930400: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:55.941268: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 15:07:56.195099: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:56.254442: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:56.261163: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:56.270210: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:07:56.282463: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round   0: OrderedDict([('sparse_categorical_accuracy', 0.010763652), ('loss', 4.125886), ('num_examples', 77483), ('num_batches', 606)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_495098/2449149110.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append({'Round': round,\n",
      "2023-03-28 15:08:01.696511: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.833760: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.840840: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.849533: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.861797: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.877817: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.888631: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.920323: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.925740: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.931294: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.935286: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.940406: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.946822: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.954609: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.972632: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:01.994438: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.008635: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.028394: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.052402: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.076292: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.079561: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.084687: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.087109: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.091310: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-28 15:08:02.115889: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Round   1: OrderedDict([('sparse_categorical_accuracy', 0.049856097), ('loss', 4.1186633), ('num_examples', 77483), ('num_batches', 606)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_495098/2449149110.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append({'Round': round,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2\n",
      "Round   2: OrderedDict([('sparse_categorical_accuracy', 0.049856097), ('loss', 4.1077724), ('num_examples', 77483), ('num_batches', 606)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_495098/2449149110.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_frame = data_frame.append({'Round': round,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noise_multiplier \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.0\u001b[39m]:\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# for noise_multiplier in [0.0, 0.5, 0.75, 1.0]:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting training with noise multiplier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoise_multiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m   data_frame \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients_per_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[44], line 69\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(rounds, noise_multiplier, clients_per_round, data_frame)\u001b[0m\n\u001b[1;32m     61\u001b[0m     sampled_train_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m         virtual_global_dataset[client] \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m sampled_clients\n\u001b[1;32m     63\u001b[0m     ]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#     sampled_train_data = [\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#         train_data.create_tf_dataset_for_client(client)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#         for client in sampled_clients]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Use selected clients for update.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_train_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     state \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m     71\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/anaconda3/envs/tff/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/computation/computation_impl.py:139\u001b[0m, in \u001b[0;36mConcreteComputation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    138\u001b[0m   arg \u001b[38;5;241m=\u001b[39m function_utils\u001b[38;5;241m.\u001b[39mpack_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_type_signature\u001b[38;5;241m.\u001b[39mparameter, args, kwargs)\n\u001b[0;32m--> 139\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tff/lib/python3.9/site-packages/tensorflow_federated/python/core/impl/execution_contexts/sync_execution_context.py:65\u001b[0m, in \u001b[0;36mSyncExecutionContext.invoke\u001b[0;34m(self, comp, arg)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, comp, arg):\n\u001b[0;32m---> 65\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_coro_and_return_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tff/lib/python3.9/site-packages/tensorflow_federated/python/common_libs/async_utils.py:224\u001b[0m, in \u001b[0;36mAsyncThreadRunner.run_coro_and_return_result\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"Runs coroutine in the managed event loop, returning the result.\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_loop)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tff/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/tff/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_frame = pd.DataFrame()\n",
    "rounds = 10\n",
    "# rounds = 50\n",
    "clients_per_round = 1000\n",
    "# rounds = 100\n",
    "# clients_per_round = 50\n",
    "\n",
    "for noise_multiplier in [0.0]:\n",
    "# for noise_multiplier in [0.0, 0.5, 0.75, 1.0]:\n",
    "  print(f'Starting training with noise multiplier: {noise_multiplier}')\n",
    "  data_frame = train(rounds, noise_multiplier, clients_per_round, data_frame)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8h5cZ2OUmUF"
   },
   "source": [
    "Now we can visualize the evaluation set accuracy and loss of those runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "EHKzgJiQSxAE"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_plot\u001b[39m(data_frame):\n\u001b[1;32m      5\u001b[0m   plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def make_plot(data_frame):\n",
    "  plt.figure(figsize=(15, 5))\n",
    "\n",
    "  dff = data_frame.rename(\n",
    "      columns={'sparse_categorical_accuracy': 'Accuracy', 'loss': 'Loss'})\n",
    "\n",
    "  plt.subplot(121)\n",
    "  sns.lineplot(data=dff, x='Round', y='Accuracy', hue='NoiseMultiplier', palette='dark')\n",
    "  plt.subplot(122)\n",
    "  sns.lineplot(data=dff, x='Round', y='Loss', hue='NoiseMultiplier', palette='dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiF8a2XxMt_8",
    "outputId": "dee18ef1-abca-40df-8bce-2d8eb6959d32"
   },
   "outputs": [],
   "source": [
    "make_plot(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jeTMaZ5MunO"
   },
   "source": [
    "It appears that with 50 expected clients per round, this model can tolerate a noise multiplier of up to 0.5 without degrading model quality. A noise multiplier of 0.75 seems to cause a bit of model degradation, and 1.0 makes the model diverge.\n",
    "\n",
    "There is typically a tradeoff between model quality and privacy. The higher noise we use, the more privacy we can get for the same amount of training time and number of clients. Conversely, with less noise, we may have a more accurate model, but we'll have to train with more clients per round to reach our target privacy level.\n",
    "\n",
    "With the experiment above, we might decide that the small amount of model deterioration at 0.75 is acceptable in order to train the final model faster, but let's assume we want to match the performance of the 0.5 noise-multiplier model.\n",
    "\n",
    "Now we can use dp_accounting functions to determine how many expected clients per round we would need to get acceptable privacy. Standard practice is to choose delta somewhat smaller than one over the number of records in the dataset. This dataset has 3383 total training users, so let's aim for (2, 1e-5)-DP.\n",
    "\n",
    "We use `dp_accounting.calibrate_dp_mechanism` to search over the number of clients per round. The privacy accountant (`RdpAccountant`) we use to estimate privacy given a `dp_accounting.DpEvent` is based on [Wang et al. (2018)](https://arxiv.org/abs/1808.00087) and [Mironov et al. (2019)](https://arxiv.org/pdf/1908.10530.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3437,
     "status": "ok",
     "timestamp": 1671496851266,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "QDMUmKWKXAMB",
    "outputId": "d32d504f-2108-41c7-9b4e-ac07e238985d"
   },
   "outputs": [],
   "source": [
    "total_clients = 3383\n",
    "noise_to_clients_ratio = 0.01\n",
    "target_delta = 1e-5\n",
    "target_eps = 2\n",
    "\n",
    "# Initialize arguments to dp_accounting.calibrate_dp_mechanism.\n",
    "\n",
    "# No-arg callable that returns a fresh accountant.\n",
    "make_fresh_accountant = dp_accounting.rdp.RdpAccountant\n",
    "\n",
    "# Create function that takes expected clients per round and returns a \n",
    "# dp_accounting.DpEvent representing the full training process.\n",
    "def make_event_from_param(clients_per_round):\n",
    "  q = clients_per_round / total_clients\n",
    "  noise_multiplier = clients_per_round * noise_to_clients_ratio\n",
    "  gaussian_event = dp_accounting.GaussianDpEvent(noise_multiplier)\n",
    "  sampled_event = dp_accounting.PoissonSampledDpEvent(q, gaussian_event)\n",
    "  composed_event = dp_accounting.SelfComposedDpEvent(sampled_event, rounds)\n",
    "  return composed_event\n",
    "\n",
    "# Create object representing the search range [1, 3383].\n",
    "bracket_interval = dp_accounting.ExplicitBracketInterval(1, total_clients)\n",
    "\n",
    "# Perform search for smallest clients_per_round achieving the target privacy.\n",
    "clients_per_round = dp_accounting.calibrate_dp_mechanism(\n",
    "    make_fresh_accountant, make_event_from_param, target_eps, target_delta,\n",
    "    bracket_interval, discrete=True\n",
    ")\n",
    "\n",
    "noise_multiplier = clients_per_round * noise_to_clients_ratio\n",
    "print(f'To get ({target_eps}, {target_delta})-DP, use {clients_per_round} '\n",
    "      f'clients with noise multiplier {noise_multiplier}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExhsP4nxiaok"
   },
   "source": [
    "Now we can train our final private model for release.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIacNzuxibOB",
    "outputId": "28fa2cc5-b760-4856-d9bc-1bbd8450f5fd"
   },
   "outputs": [],
   "source": [
    "rounds = 100\n",
    "noise_multiplier = 1.2\n",
    "clients_per_round = 120\n",
    "\n",
    "data_frame = pd.DataFrame()\n",
    "data_frame = train(rounds, noise_multiplier, clients_per_round, data_frame)\n",
    "\n",
    "make_plot(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-N7cvpVkFR6"
   },
   "source": [
    "As we can see, the final model has similar loss and accuracy to the model trained without noise, but this one satisfies (2, 1e-5)-DP."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "federated_learning_with_differential_privacy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tff)",
   "language": "python",
   "name": "tff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
